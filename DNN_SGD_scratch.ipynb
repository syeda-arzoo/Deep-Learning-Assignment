{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DL_hw_prob2_prob4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDnTxNFiBlCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "df = pd.read_csv ('train_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osH1yJPsBlCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split the output class label\n",
        "df['class1'] = (df[' class'] == 0)\n",
        "df.class1 = df.class1.astype(int)\n",
        "X = df[[' x1',' x2']].values\n",
        "y = df[[' class','class1']].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z67ESZZlBlDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(z):\n",
        "    exp_scores = np.exp(z)\n",
        "    return (exp_scores / np.sum(exp_scores))\n",
        "\n",
        "def dsoftmax(z):\n",
        "    exp_scores = np.exp(z)\n",
        "    s = (exp_scores / np.sum(exp_scores))\n",
        "    return (s * (1-s))\n",
        "\n",
        "def loss(y,y_hat):\n",
        "    loss = np.sum((y-y_hat)**2)\n",
        "    return loss\n",
        "\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def dSigmoid(Z):\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    return (s * (1-s))\n",
        "\n",
        "# Forward pass function\n",
        "def forward_prop(model,a0):  \n",
        "    # Load parameters from model\n",
        "    W1, W2, W3 = model['W1'], model['W2'], model['W3']\n",
        "    z1 = a0.dot(W1)\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = a1.dot(W2)\n",
        "    a2 = sigmoid(z2)\n",
        "    z3 = a2.dot(W3)\n",
        "    a3 = softmax(z3)\n",
        "    #Store all results in cache\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache\n",
        "\n",
        "# Backpropagation\n",
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    W1, W2, W3 = model['W1'], model['W2'], model['W3']\n",
        "    \n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "\n",
        "    dz3 = np.multiply(loss_derivative(y=y,y_hat=a3),dsoftmax(a3))\n",
        "\n",
        "    dW3 = (a2.T).dot(dz3) \n",
        "\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,dSigmoid(a2))\n",
        "\n",
        "    dW2 = np.dot(a1.T, dz2)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),dSigmoid(a1))\n",
        "\n",
        "    dW1 = np.dot(a0.T,dz1)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'dW2':dW2, 'dW1':dW1}\n",
        "    return grads\n",
        "\n",
        "# Training\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    \n",
        "    model = { 'W1': W1, 'W2': W2,'W3':W3}\n",
        "    return model\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, W2, W3 = model['W1'], model['W2'], model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'W2': W2, 'W3':W3}\n",
        "    return model\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat\n",
        "\n",
        "losses = []\n",
        "def train(model,X_,y_,learning_rate, epochs=50, print_loss=False):\n",
        "    # Stochastic Gradient descent. Loop over epochs\n",
        "    for i in range(0, epochs):\n",
        "        # Update weights after every sample\n",
        "        for k in range(0,999):\n",
        "                \n",
        "            # Forward propagation\n",
        "            Xsample = np.reshape(X_[k],(1,2))            \n",
        "            cache = forward_prop(model,Xsample)\n",
        "\n",
        "            # Backpropagation            \n",
        "            Ysample = np.reshape(y_[k],(1,2))\n",
        "            grads = backward_prop(model,cache,Ysample)\n",
        "            \n",
        "            # SGD parameter update\n",
        "            model = update_parameters(model=model,grads=grads,learning_rate=learning_rate) \n",
        "            \n",
        "            # Pring loss & accuracy every 100 iterations\n",
        "            if k == 998:\n",
        "                    a3 = cache['a3']\n",
        "                    print('Loss after epoch',i,':',loss(Ysample,a3))\n",
        "                    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yenQm0BABlDN",
        "colab_type": "code",
        "colab": {},
        "outputId": "a41d56de-2c82-4e56-90d5-75d3a55982cd"
      },
      "source": [
        "np.random.seed(0)\n",
        "# This is what we return at the end\n",
        "model = initialize_parameters(nn_input_dim=2, nn_hdim= 3, nn_output_dim= 2)\n",
        "model = train(model,X,y,learning_rate=0.01,epochs=50,print_loss=True)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after epoch 0 : 0.5228850135421004\n",
            "Loss after epoch 1 : 0.4251828402169788\n",
            "Loss after epoch 2 : 0.37310619409685475\n",
            "Loss after epoch 3 : 0.326079091975328\n",
            "Loss after epoch 4 : 0.27311489917870213\n",
            "Loss after epoch 5 : 0.21554998415882592\n",
            "Loss after epoch 6 : 0.16066229889834419\n",
            "Loss after epoch 7 : 0.11517640146437469\n",
            "Loss after epoch 8 : 0.08121017924353074\n",
            "Loss after epoch 9 : 0.0572482634838678\n",
            "Loss after epoch 10 : 0.04082747060848101\n",
            "Loss after epoch 11 : 0.029702337840705043\n",
            "Loss after epoch 12 : 0.02209843233694466\n",
            "Loss after epoch 13 : 0.016805604748765933\n",
            "Loss after epoch 14 : 0.01304632034364529\n",
            "Loss after epoch 15 : 0.010322110772377345\n",
            "Loss after epoch 16 : 0.008308568127657748\n",
            "Loss after epoch 17 : 0.006791749237836165\n",
            "Loss after epoch 18 : 0.005628642397660649\n",
            "Loss after epoch 19 : 0.004722105094944911\n",
            "Loss after epoch 20 : 0.004004977666636773\n",
            "Loss after epoch 21 : 0.003429991690826443\n",
            "Loss after epoch 22 : 0.002963301678659768\n",
            "Loss after epoch 23 : 0.002580281042841053\n",
            "Loss after epoch 24 : 0.0022627437198672606\n",
            "Loss after epoch 25 : 0.001997074748342823\n",
            "Loss after epoch 26 : 0.0017729497291882547\n",
            "Loss after epoch 27 : 0.0015824431498001304\n",
            "Loss after epoch 28 : 0.0014193989058425148\n",
            "Loss after epoch 29 : 0.001278981068703433\n",
            "Loss after epoch 30 : 0.0011573503369579948\n",
            "Loss after epoch 31 : 0.0010514288477878409\n",
            "Loss after epoch 32 : 0.0009587273988105594\n",
            "Loss after epoch 33 : 0.0008772169699221702\n",
            "Loss after epoch 34 : 0.0008052319396336011\n",
            "Loss after epoch 35 : 0.0007413962384482505\n",
            "Loss after epoch 36 : 0.000684566334085406\n",
            "Loss after epoch 37 : 0.0006337867506484498\n",
            "Loss after epoch 38 : 0.0005882550520946228\n",
            "Loss after epoch 39 : 0.0005472940609752854\n",
            "Loss after epoch 40 : 0.0005103296670851885\n",
            "Loss after epoch 41 : 0.0004768729934382141\n",
            "Loss after epoch 42 : 0.0004465059845318668\n",
            "Loss after epoch 43 : 0.00041886970015099425\n",
            "Loss after epoch 44 : 0.0003936547605354133\n",
            "Loss after epoch 45 : 0.00037059351132708035\n",
            "Loss after epoch 46 : 0.0003494535700866617\n",
            "Loss after epoch 47 : 0.00033003248787504105\n",
            "Loss after epoch 48 : 0.0003121533148380458\n",
            "Loss after epoch 49 : 0.00029566090185337675\n",
            "{'W1': array([[ -8.78560908, -10.16917447,   4.57445751],\n",
            "       [ 14.13915694,  12.10293852,  -6.39311299]]), 'W2': array([[ 2.10811656, -4.64217496, -0.7811449 ],\n",
            "       [ 1.17091238, -4.36168391,  2.26813443],\n",
            "       [-1.00876943,  2.5156598 , -0.52231438]]), 'W3': array([[ 2.36117594, -0.80562213],\n",
            "       [-2.85199275,  3.33600982],\n",
            "       [ 0.58362516, -1.06723427]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAzcfLRSBlDV",
        "colab_type": "code",
        "colab": {},
        "outputId": "ecbf99af-cf93-49ea-a819-ca8465f91a4c"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "df_test = pd.read_csv (r'/Users/arzoo/Desktop/SEM1/Intro to DL/test_data.csv')\n",
        "X_test = df_test[[' x1',' x2']].values\n",
        "df_test['class1'] = (df_test[' class'] == 0)\n",
        "df_test.class1 = df_test.class1.astype(int)\n",
        "y_test = df[[' class','class1']].values\n",
        "y_hat = predict(model,X_test)\n",
        "y_true = y_test.argmax(axis=1)\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "print(\"precision: \", precision_score(y_true, y_hat , average=\"macro\"))\n",
        "print(\"recall: \", recall_score(y_true, y_hat , average=\"macro\"))\n",
        "print(\"f1 score: \", f1_score(y_true, y_hat , average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision:  0.999\n",
            "recall:  0.999\n",
            "f1 score:  0.998998998998999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3ijhK06BlDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}